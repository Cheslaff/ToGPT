{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L0GiMTR0ppp_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "exmBApl_1J94"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"data.txt\", \"r\").read()\n",
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
        "itos = {i:ch for ch, i in stoi.items()}"
      ],
      "metadata": {
        "id": "T7EW3y-ysW9c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.rt = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.rtgate = nn.Sigmoid()\n",
        "    self.zt = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.ztgate = nn.Sigmoid()\n",
        "    self.ht = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.htact = nn.Tanh()\n",
        "    self.o = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def init_hidden(self, device):\n",
        "    return torch.zeros((1, self.hidden_size), device=device)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    inp = torch.cat((input, hidden), dim=1)\n",
        "    rt = self.rtgate(self.rt(inp))\n",
        "    zt = self.ztgate(self.zt(inp))\n",
        "    hid_rt = hidden * rt\n",
        "    candidate_inp = torch.cat((input, hid_rt), dim=1)\n",
        "    ht = self.htact(self.ht(candidate_inp))\n",
        "    new_hidden = (1 - zt) * ht + zt * hidden\n",
        "    output = self.o(new_hidden)\n",
        "\n",
        "    return output, new_hidden"
      ],
      "metadata": {
        "id": "BSOalrEvtw0E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = output_size = vocab_size\n",
        "hiddens = 96\n",
        "\n",
        "gru = GRU(input_size, hiddens, output_size).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0EMdvxtUxTeP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(start=\"\", len_=20):\n",
        "  chars = [stoi[start]]\n",
        "  hidden = gru.init_hidden(device)\n",
        "  for i in range(len_):\n",
        "    inp = torch.tensor(chars[-1]).unsqueeze(0)\n",
        "    inp = F.one_hot(inp, vocab_size).to(device)\n",
        "    logits, hidden = gru(inp, hidden)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    ix = torch.multinomial(probs, 1).item()\n",
        "    chars.append(ix)\n",
        "  return \"\".join(itos[ix] for ix in chars)"
      ],
      "metadata": {
        "id": "z8iwlBQt2hSq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "e4bcFCKC7S7v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_window = 24\n",
        "chunk_size = 2000\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  ix = torch.randint(0, len(text)-chunk_size-2, size=(1,)).item()\n",
        "  for i in tqdm(range(ix, ix+chunk_size-context_window)):\n",
        "    chunk = text[i:i+context_window]\n",
        "    target = text[i+1:i+1+context_window]\n",
        "    chunk = torch.tensor([stoi[ch] for ch in chunk])\n",
        "    target = [stoi[ch] for ch in target]\n",
        "    chunk = F.one_hot(chunk, vocab_size).to(device)\n",
        "    target = torch.tensor(target).to(device)\n",
        "\n",
        "    hidden = gru.init_hidden(device)\n",
        "    total_loss = 0\n",
        "    for x in range(chunk.shape[0]):\n",
        "      inp = chunk[x].unsqueeze(0)\n",
        "      tg = target[x].unsqueeze(0)\n",
        "      logits, hidden = gru(inp, hidden)\n",
        "      loss = loss_fn(logits, tg)\n",
        "      total_loss += loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "  if (epoch+1) % 50 == 0:  # bruh\n",
        "    print(total_loss.item())\n",
        "    print(generate(random.choice(vocab), 300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB3sT6FTxsaA",
        "outputId": "a8bc65fa-d32d-4442-fe1a-fe1f382c90c3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1976/1976 [01:05<00:00, 30.36it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 30.96it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 30.90it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.16it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.35it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.59it/s]\n",
            "100%|██████████| 1976/1976 [01:01<00:00, 31.91it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.65it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.70it/s]\n",
            "100%|██████████| 1976/1976 [01:04<00:00, 30.86it/s]\n",
            "100%|██████████| 1976/1976 [01:05<00:00, 30.07it/s]\n",
            "100%|██████████| 1976/1976 [01:04<00:00, 30.85it/s]\n",
            "100%|██████████| 1976/1976 [01:04<00:00, 30.68it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.38it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.04it/s]\n",
            "100%|██████████| 1976/1976 [01:05<00:00, 30.05it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.56it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.47it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.24it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.31it/s]\n",
            "100%|██████████| 1976/1976 [01:04<00:00, 30.46it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.64it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.69it/s]\n",
            "100%|██████████| 1976/1976 [01:01<00:00, 32.00it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.79it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.41it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.85it/s]\n",
            "100%|██████████| 1976/1976 [01:01<00:00, 31.88it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.38it/s]\n",
            "100%|██████████| 1976/1976 [01:06<00:00, 29.58it/s]\n",
            "100%|██████████| 1976/1976 [01:06<00:00, 29.59it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 30.91it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.22it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 30.90it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.58it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.62it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.64it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.21it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.04it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.67it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.21it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.51it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.52it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.45it/s]\n",
            "100%|██████████| 1976/1976 [01:03<00:00, 31.22it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.54it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.38it/s]\n",
            "100%|██████████| 1976/1976 [01:04<00:00, 30.86it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.56it/s]\n",
            "100%|██████████| 1976/1976 [01:02<00:00, 31.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49.757843017578125\n",
            "мелими нащим изи чём нек за, за в ду и ещё бы не завернял ял ни валерна валаж.\n",
            "Я вет за ращищи вид и наветпяля от моёсо вдмикизна бок в ото!\n",
            "Вы, чё денова. Броволли и не вадми- не ещёоти с смото межкимо- из купить? прогледя я в порокам пимой из нупадибо могли омовя навибо грася терпаящий прий нискост\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(gru.state_dict(), \"/content/gru_params.pth\")"
      ],
      "metadata": {
        "id": "07rHzcmKIEfG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(\"Д\", 1_000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHQwhBxf12SI",
        "outputId": "2c7ed7a2-a3f3-4abf-bc45-015f7275ea89"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Да, коконкогодить, мочканом дирововня и порока к не взгли. Я намилк и видняной вез но! Я занил Джону ве и и настолми комнуе. Дорупалошким ос ещё ил и отющибуборащи.\n",
            "Полета дола и пароком.\n",
            "Я вар.\n",
            "Я градиз споорносто очкосчавачеравидяты, чтобот. Потой от стой, вы ну, забиравива протновило в вернимокну межния стоу верпока.\n",
            "Нока мне. Того граят от. Подой, и визбудее каком и простоиби сегода.. мне и из бартела, вои и идно тобобкя они отдыпивастьет, !\n",
            "Я и из убирника я вадот. Лооящия  вожк прощенома вся писье запяровив скоДупорока и вернула каки на оты сайдя ули мне гори в мела\n",
            "ми отой каком идилизмолое вст соба и До, запикий дасчу, не звисниз нате мёнча, из на пожил видяибийда визнивернавили в дудо вабот, к оновилько мрисли:\n",
            "У нам хотолжиошь отдиби да мой идя он ношо. ли васпраглечто можно комне обятьке забанная расотне, звароки, монянна ми кечкомолжазбили дом, исти в мниме могом доми-тлоятьсти и кононь л жиное стерупимеми отрющая саровенила видо ни к из вись тыбивший и ведсотот вваероко бор\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Well, looks like Glorps language, transfers style. Well done!\n",
        "<img src=\"https://media.tenor.com/sEiYXWmf1W8AAAAi/glorp-alien.gif\">"
      ],
      "metadata": {
        "id": "xeseedDJI4ow"
      }
    }
  ]
}